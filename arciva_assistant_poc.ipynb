{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header_md",
   "metadata": {},
   "source": [
    "# Arciva — LLM Assistant Proof of Concept\n",
    "\n",
    "## Executive Summary\n",
    "Arciva is a cross-platform photo library manager built with a FastAPI backend, React frontend, and a Rust-based processing core. This notebook documents the feasibility study for an in-app assistant that can answer workflow questions, guide photographers through Arciva's UI, and surface context-specific help.\n",
    "\n",
    "The goal is to show that a compact model, fine-tuned with QLoRA, can internalize Arciva terminology and deliver useful assistant behavior while remaining deployable on edge hardware (e.g., Jetson-class devices). The pipeline covers end-to-end steps needed for a reproducible prototype:\n",
    "1.  **Data Ingestion & Validation**: Load the synthetic Arciva Q&A corpus, perform sanity checks, and create evaluation splits.\n",
    "2.  **Efficient Training**: Apply QLoRA adapters to a 7B base model (or an optional tiny debug model) for rapid iteration without large compute budgets.\n",
    "3.  **Monitoring & Diagnostics**: Visualize training progress inside the notebook to catch regressions early.\n",
    "4.  **Quantitative & Qualitative Review**: Compute PPL/BLEU/ROUGE and inspect representative generations to gauge assistant quality.\n",
    "5.  **Interactive Exploration**: Provide a lightweight dashboard for browsing predictions across dataset splits.\n",
    "6.  **Artifact Preparation**: Merge adapters back into the base weights and stage artifacts for downstream integration tests.\n",
    "\n",
    "This artifact is included in Arciva applications to demonstrate practical LLM fine-tuning skills, data quality awareness, and a deployment-focused mindset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_header",
   "metadata": {},
   "source": [
    "## 1. Environment Configuration\n",
    "This cell pins the Python dependencies required for the Arciva assistant prototype. Keeping installation commands at the top ensures Colab/RunPod sessions can be recreated quickly and makes debugging import errors straightforward. If `typing_extensions` or `pydantic` raise issues, restart the runtime and rerun this setup cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Core Dependency Upgrades (Must run first to avoid conflicts)\n",
    "!pip install -U -q 'typing_extensions>=4.12.2' pydantic\n",
    "\n",
    "# 2. ML Pipeline Installation\n",
    "!pip install -q transformers peft trl bitsandbytes accelerate wandb scipy datasets matplotlib evaluate rouge_score sacrebleu pandas seaborn\n",
    "\n",
    "import os\n",
    "print(\"[SYSTEM] Environment Configuration Complete.\")\n",
    "print(\"[NOTE] If you see an import error below, please restart the runtime.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports_header",
   "metadata": {},
   "source": [
    "## 2. Library Imports & Global Configuration\n",
    "All core libraries for fine-tuning, evaluation, and visualization are imported here. Global switches (e.g., enabling a tiny debug model) control whether we run a fast smoke test or a full QLoRA pass on the Arciva dataset, keeping experiments reproducible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imports_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import time\n",
    "import transformers\n",
    "import trl\n",
    "import peft\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sacrebleu\n",
    "from rouge_score import rouge_scorer\n",
    "from IPython.display import clear_output, display\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainerCallback,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, PeftModel\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "try:\n",
    "    from google.colab import data_table\n",
    "    HAS_COLAB = True\n",
    "except ModuleNotFoundError:\n",
    "    data_table = None\n",
    "    HAS_COLAB = False\n",
    "    print('[WARN] google.colab module not found; interactive tables disabled.')\n",
    "\n",
    "# --- Global Hyperparameters ---\n",
    "BASE_MODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\n",
    "FAST_DEBUG_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Tiny model with safetensors for quick smoke tests\n",
    "USE_DEBUG_MODEL = False  # Set True to run rapid dummy training/evals\n",
    "MODEL_NAME = FAST_DEBUG_MODEL if USE_DEBUG_MODEL else BASE_MODEL_NAME\n",
    "DATA_PATH = \"arciva_qa_synthetic.jsonl\"\n",
    "OUTPUT_DIR = \"model_output\"\n",
    "FINAL_SAVE_DIR = \"final_merged_model\"\n",
    "USE_WANDB = False \n",
    "# Training Hyperparameters\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 2e-4\n",
    "MAX_SEQ_LENGTH = 512\n",
    "TRAIN_BATCH_SIZE_GPU = 4\n",
    "TRAIN_BATCH_SIZE_CPU = 1\n",
    "GRAD_ACCUM_STEPS_GPU = 1\n",
    "GRAD_ACCUM_STEPS_CPU = 4\n",
    "LOGGING_STEPS = 10\n",
    "EVAL_STEPS = 10\n",
    "\n",
    "print(f\"{'-'*40}\")\n",
    "if USE_DEBUG_MODEL:\n",
    "    print('[DEBUG] Fast debug model enabled; switch USE_DEBUG_MODEL to False for full runs.')\n",
    "print(f\"Target Architecture : {MODEL_NAME}\")\n",
    "print(f\"Compute Device      : {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"{'-'*40}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_analysis_header",
   "metadata": {},
   "source": [
    "## 3. Data Ingestion & Preprocessing\n",
    "Load the synthetic Arciva assistant corpus (~1.2k question/answer pairs), perform minimal schema checks, and create splits for training (≈81%), validation (≈9%), and testing (≈10%). This mirrors the workflow we will use once higher-quality labeled data from the Arciva tagging tool becomes available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "data_analysis_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    if os.path.exists(DATA_PATH):\n",
    "        print(f\"[INFO] Loading dataset from source: {DATA_PATH}...\")\n",
    "        full_dataset = load_dataset(\"json\", data_files=DATA_PATH, split=\"train\")\n",
    "    else:\n",
    "        print(f\"[WARN] Source file {DATA_PATH} unavailable. Generating synthetic verification data.\")\n",
    "        full_dataset = Dataset.from_dict({\n",
    "            \"instruction\": [\"Explain quantum mechanics.\", \"How do I reset my password?\"] * 50,\n",
    "            \"response\": [\"It is complex.\", \"Go to settings and click reset.\"] * 50\n",
    "        })\n",
    "\n",
    "    # Stratified Split (Approximated by random shuffle)\n",
    "    # Split 1: 10% Test\n",
    "    split_1 = full_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "    test_set = split_1[\"test\"]\n",
    "    remaining = split_1[\"train\"]\n",
    "\n",
    "    # Split 2: 10% Validation (of remaining -> ~9% of total)\n",
    "    split_2 = remaining.train_test_split(test_size=0.1, seed=42)\n",
    "    train_set = split_2[\"train\"]\n",
    "    val_set = split_2[\"test\"]\n",
    "\n",
    "    return train_set, val_set, test_set\n",
    "\n",
    "def inspect_data_distribution(dataset, name=\"Training Set\"):\n",
    "    # Convert to Pandas for Analysis\n",
    "    df = dataset.to_pandas()\n",
    "    \n",
    "    # Robust Length Calculation (Handling missing keys)\n",
    "    df['inst_len'] = df['instruction'].fillna(\"\").apply(lambda x: len(str(x).split()))\n",
    "    # specific handling for response/output duality\n",
    "    if 'response' in df.columns:\n",
    "        df['resp_len'] = df['response'].fillna(\"\").apply(lambda x: len(str(x).split()))\n",
    "    elif 'output' in df.columns:\n",
    "         df['resp_len'] = df['output'].fillna(\"\").apply(lambda x: len(str(x).split()))\n",
    "    else:\n",
    "        df['resp_len'] = 0\n",
    "\n",
    "    # Visualization\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot 1: Instruction Length\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(df['inst_len'], color=\"#1f77b4\", kde=True)\n",
    "    plt.title(f\"{name}: Instruction Length Distribution\")\n",
    "    plt.xlabel(\"Word Count\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    \n",
    "    # Plot 2: Response Length\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(df['resp_len'], color=\"#2a9d8f\", kde=True)\n",
    "    plt.title(f\"{name}: Response Length Distribution\")\n",
    "    plt.xlabel(\"Word Count\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "train_dataset, eval_dataset, test_dataset = prepare_data()\n",
    "\n",
    "# --- Data Integrity Inspection ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"            DATASET STATISTICS            \")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total Corpus Size  : {len(train_dataset) + len(eval_dataset) + len(test_dataset)} samples\")\n",
    "print(f\"Training Subset    : {len(train_dataset)} samples\")\n",
    "print(f\"Validation Subset  : {len(eval_dataset)} samples\")\n",
    "print(f\"Test Subset        : {len(test_dataset)} samples\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "print(\"\\n[DATA SAMPLE: Index 0]\")\n",
    "sample = train_dataset[0]\n",
    "print(f\"> INSTRUCTION:\\n{sample.get('instruction')}\")\n",
    "print(f\"\\n> RESPONSE:\\n{sample.get('response') or sample.get('output')}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# --- Distribution Visualization ---\n",
    "print(\"\\n[ANALYSIS] Visualizing Training Corpus Distribution...\")\n",
    "inspect_data_distribution(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_header",
   "metadata": {},
   "source": [
    "## 4. Model Fine-Tuning\n",
    "Fine-tune the chosen base model (Mistral-7B or the optional tiny debug checkpoint) with QLoRA adapters. This keeps compute light, mirrors the hardware constraints we expect on Arciva edge devices, and still lets the model absorb Arciva-specific navigation, tagging, and workflow vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "training_utils",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-Time Loss Visualization Callback\n",
    "class NotebookPlotCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.steps = []\n",
    "        self.val_steps = []\n",
    "        self.start_time = None\n",
    "\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs:\n",
    "            if 'loss' in logs:\n",
    "                self.losses.append(logs['loss'])\n",
    "                self.steps.append(state.global_step)\n",
    "            if 'eval_loss' in logs:\n",
    "                self.val_losses.append(logs['eval_loss'])\n",
    "                self.val_steps.append(state.global_step)\n",
    "            \n",
    "            if 'loss' in logs or 'eval_loss' in logs:\n",
    "                # Calculate Estimated Time of Arrival (ETA)\n",
    "                elapsed = time.time() - self.start_time\n",
    "                avg_time_per_step = elapsed / state.global_step if state.global_step > 0 else 0\n",
    "                remaining_steps = state.max_steps - state.global_step\n",
    "                eta_seconds = int(avg_time_per_step * remaining_steps)\n",
    "                eta_str = f\"{eta_seconds // 60}m {eta_seconds % 60}s\"\n",
    "\n",
    "                clear_output(wait=True)\n",
    "                sns.set_theme(style=\"whitegrid\")\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                if self.losses:\n",
    "                    plt.plot(self.steps, self.losses, label='Training Loss', color='#1f77b4', linewidth=2)\n",
    "                if self.val_losses:\n",
    "                    plt.plot(self.val_steps, self.val_losses, label='Validation Loss', color='#d62728', marker='o', linewidth=2)\n",
    "                plt.xlabel('Training Steps', fontsize=10)\n",
    "                plt.ylabel('Loss Value', fontsize=10)\n",
    "                plt.title(f'Training Convergence Dynamics (ETA: {eta_str})', fontsize=12, fontweight='bold')\n",
    "                plt.legend()\n",
    "                plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "                plt.show()\n",
    "\n",
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    instruction = example.get('instruction')\n",
    "    response = example.get('response') or example.get('output')\n",
    "    if isinstance(instruction, list):\n",
    "        for i, r in zip(instruction, response):\n",
    "            output_texts.append(f\"### Instruction:\\n{i}\\n\\n### Response:\\n{r}\")\n",
    "        return output_texts\n",
    "    return f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{response}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "training_execution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_pipeline(train_data, val_data):\n",
    "    # Hardware Acceleration Logic\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    compute_dtype = torch.float16\n",
    "    bf16_kwargs = {\"bf16\": False, \"fp16\": True}\n",
    "    \n",
    "    if use_gpu and torch.cuda.is_bf16_supported():\n",
    "        print(\"[INFO] BF16 acceleration enabled for compatible hardware.\")\n",
    "        compute_dtype = torch.bfloat16\n",
    "        bf16_kwargs = {\"bf16\": True, \"fp16\": False}\n",
    "\n",
    "    # 4-bit Quantization Configuration\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "    ) if use_gpu else None\n",
    "\n",
    "    print(\"[INFO] Initializing Base Model...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\" if use_gpu else \"cpu\",\n",
    "        trust_remote_code=True,\n",
    "        use_safetensors=True,\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Prepare for LoRA\n",
    "    model = prepare_model_for_kbit_training(model) if use_gpu else model\n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=16, lora_dropout=0.1, r=64, bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "\n",
    "    # Training Hyperparameters\n",
    "    args = SFTConfig(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=TRAIN_BATCH_SIZE_GPU if use_gpu else TRAIN_BATCH_SIZE_CPU,\n",
    "        gradient_accumulation_steps=GRAD_ACCUM_STEPS_GPU if use_gpu else GRAD_ACCUM_STEPS_CPU,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        logging_steps=LOGGING_STEPS,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=EVAL_STEPS,\n",
    "        report_to=\"none\",\n",
    "        use_cpu=not use_gpu,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        **bf16_kwargs\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        peft_config=peft_config,\n",
    "        formatting_func=formatting_prompts_func,\n",
    "        processing_class=tokenizer,\n",
    "        args=args,\n",
    "        callbacks=[NotebookPlotCallback()]\n",
    "    )\n",
    "\n",
    "    print(\"[INFO] Starting Training Loop...\")\n",
    "    trainer.train()\n",
    "    trainer.model.save_pretrained(OUTPUT_DIR)\n",
    "    print(f\"[INFO] Training concluded. Artifacts saved to: {OUTPUT_DIR}\")\n",
    "    \n",
    "    # Memory Cleanup\n",
    "    del model, trainer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "run_training_pipeline(train_dataset, eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation_header",
   "metadata": {},
   "source": [
    "## 5. Quantitative Analysis & Reporting\n",
    "Compute lightweight metrics on the held-out test set to understand whether the assistant is learning Arciva patterns at all. Perplexity tracks language modeling fit, BLEU/ROUGE-L capture instruction-following fidelity, and the surrounding visualization surfaces regression risks before we scale to a larger dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "evaluation_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_metrics(train_data, test_data):\n",
    "    print(\"[INFO] Reloading model architecture for inference...\")\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "\n",
    "    # Inference Init (FP16/4-bit)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        load_in_4bit=True if use_gpu else False,\n",
    "        device_map=\"auto\" if use_gpu else \"cpu\",\n",
    "        trust_remote_code=True,\n",
    "        use_safetensors=True,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n",
    "    model.eval()\n",
    "    \n",
    "    # --- 1. Perplexity Calculation ---\n",
    "    print(\"[METRICS] Calculating Perplexity...\")\n",
    "    eval_args = TrainingArguments(\n",
    "        output_dir=\"temp_eval\", \n",
    "        per_device_eval_batch_size=4, \n",
    "        report_to=\"none\"\n",
    "    )\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data, \n",
    "        eval_dataset=test_data,\n",
    "        formatting_func=formatting_prompts_func,\n",
    "        processing_class=tokenizer,\n",
    "        args=eval_args\n",
    "    )\n",
    "    results = trainer.evaluate()\n",
    "    loss = results[\"eval_loss\"]\n",
    "    perp = torch.exp(torch.tensor(loss))\n",
    "    \n",
    "    # --- 2. Generation & Similarity Analysis ---\n",
    "    print(\"[METRICS] Generating responses for semantic analysis...\")\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    \n",
    "    results_data = []\n",
    "    all_preds = []\n",
    "    all_refs = []\n",
    "    \n",
    "    for i, item in enumerate(test_data):\n",
    "        prompt = f\"### Instruction:\\n{item['instruction']}\\n\\n### Response:\\n\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=100, \n",
    "                do_sample=True, \n",
    "                top_p=0.9, \n",
    "                temperature=0.7,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        gen_text = tokenizer.decode(out[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()\n",
    "        ref_text = (item.get('response') or item.get('output')).strip()\n",
    "        \n",
    "        all_preds.append(gen_text)\n",
    "        all_refs.append(ref_text)\n",
    "        \n",
    "        score = scorer.score(ref_text, gen_text)['rougeL'].fmeasure\n",
    "        \n",
    "        results_data.append({\n",
    "            \"Instruction\": item['instruction'],\n",
    "            \"Prediction\": gen_text,\n",
    "            \"Reference\": ref_text,\n",
    "            \"ROUGE-L\": score,\n",
    "            \"Pred_Len\": len(gen_text.split()),\n",
    "            \"Ref_Len\": len(ref_text.split())\n",
    "        })\n",
    "\n",
    "    # Calculate Corpus BLEU\n",
    "    bleu = sacrebleu.corpus_bleu(all_preds, [all_refs])\n",
    "    \n",
    "    df_results = pd.DataFrame(results_data)\n",
    "    \n",
    "    # --- Visualization Panel (Saved to Disk) ---\n",
    "    print(\"\\n[INFO] Saving visualization panel to 'analysis_plots.png'...\")\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    sns.histplot(df_results[\"ROUGE-L\"], bins=10, kde=True, ax=ax[0], color=\"#2a9d8f\")\n",
    "    ax[0].set_title(\"Distribution of Semantic Similarity (ROUGE-L)\", fontweight='bold')\n",
    "    \n",
    "    df_melt = df_results.melt(value_vars=[\"Pred_Len\", \"Ref_Len\"], var_name=\"Source\", value_name=\"Length (Tokens)\")\n",
    "    sns.boxplot(x=\"Source\", y=\"Length (Tokens)\", hue=\"Source\", data=df_melt, ax=ax[1], palette=\"viridis\", legend=False)\n",
    "    ax[1].set_title(\"Generation Length Analysis\", fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # --- Executive Summary Table ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"             PERFORMANCE SUMMARY             \")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"{ 'Metric':<20} | { 'Value':<10}\")\n",
    "    print(\"-\"*40)\n",
    "    print(f\"{ 'Test Loss':<20} | {loss:.4f}\")\n",
    "    print(f\"{ 'Perplexity':<20} | {perp:.4f}\")\n",
    "    print(f\"{ 'BLEU Score':<20} | {bleu.score:.2f}\")\n",
    "    print(f\"{ 'Avg ROUGE-L':<20} | {df_results['ROUGE-L'].mean():.4f}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(\"\\n[ANALYSIS] Top Performing Generations:\")\n",
    "    display(df_results.nlargest(3, \"ROUGE-L\")[[\"Instruction\", \"Prediction\", \"ROUGE-L\"]])\n",
    "    \n",
    "    print(\"\\n[ANALYSIS] Low Performing Generations:\")\n",
    "    display(df_results.nsmallest(3, \"ROUGE-L\")[[\"Instruction\", \"Prediction\", \"ROUGE-L\"]])\n",
    "\n",
    "    # Pre-Merge Cleanup\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "evaluate_model_metrics(train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dashboard_header",
   "metadata": {},
   "source": [
    "## 6. Interactive Performance Dashboard\n",
    "Sample generations across Train/Validation/Test splits to inspect strengths and failure modes. The table view mimics how Arciva's eventual in-product assistant review tooling could look—search by topic, sort by quality, and compare references to predictions when curating new training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dashboard_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# google.colab data_table handled in imports cell\n",
    "\n",
    "def generate_interactive_dashboard(train_set, val_set, test_set, n_samples=20):\n",
    "    print(f\"[DASHBOARD] Initializing Interactive Analysis (Sampling N={n_samples} per split)...\")\n",
    "    \n",
    "    # Reload architecture for clean inference context\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME, \n",
    "        load_in_4bit=True if use_gpu else False, \n",
    "        device_map=\"auto\" if use_gpu else \"cpu\", \n",
    "        trust_remote_code=True,\n",
    "        use_safetensors=True,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n",
    "    model.eval()\n",
    "\n",
    "    splits = {\n",
    "        \"Train\": train_set,\n",
    "        \"Validation\": val_set,\n",
    "        \"Test\": test_set\n",
    "    }\n",
    "    \n",
    "    dashboard_data = []\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "    for split_name, dataset in splits.items():\n",
    "        # Safe Random Sampling\n",
    "        n = min(n_samples, len(dataset))\n",
    "        subset = dataset.shuffle(seed=42).select(range(n))\n",
    "        \n",
    "        print(f\"> Processing split: {split_name} ({n} items)...\")\n",
    "        for item in subset:\n",
    "            prompt = f\"### Instruction:\\n{item['instruction']}\\n\\n### Response:\\n\"\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                out = model.generate(\n",
    "                    **inputs, \n",
    "                    max_new_tokens=100, \n",
    "                    do_sample=True, \n",
    "                    top_p=0.9, \n",
    "                    temperature=0.7,\n",
    "                    pad_token_id=tokenizer.pad_token_id\n",
    "                )\n",
    "            \n",
    "            gen_text = tokenizer.decode(out[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()\n",
    "            ref_text = (item.get('response') or item.get('output')).strip()\n",
    "            score = scorer.score(ref_text, gen_text)['rougeL'].fmeasure\n",
    "            \n",
    "            dashboard_data.append({\n",
    "                \"Split\": split_name,\n",
    "                \"Instruction\": item['instruction'],\n",
    "                \"Reference\": ref_text,\n",
    "                \"Prediction\": gen_text,\n",
    "                \"ROUGE-L\": round(score, 4)\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(dashboard_data)\n",
    "    \n",
    "    # Render Interactive Table\n",
    "    print(\"[SUCCESS] Dashboard generated. Use the table below to Sort, Filter, and Search.\")\n",
    "    if HAS_COLAB and data_table is not None:\n",
    "        print('[INFO] Rendering interactive DataTable (Colab only).')\n",
    "        data_table.enable_dataframe_formatter()\n",
    "        display(data_table.DataTable(df, include_index=False, num_rows_per_page=10))\n",
    "    else:\n",
    "        print('[INFO] google.colab data_table unavailable; showing pandas preview instead.')\n",
    "        display(df.head(min(10, len(df))))\n",
    "    \n",
    "    # Cleanup\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return df\n",
    "\n",
    "generate_interactive_dashboard(train_dataset, eval_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "merge_header",
   "metadata": {},
   "source": [
    "## 7. Artifact Compilation & Export\n",
    "Merge the trained adapters into the fp16 backbone to create a portable checkpoint. These artifacts are the starting point for Arciva's edge deployment experiments (quantization, Jetson benchmarking, and FastAPI integration tests).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "merge_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_final_model():\n",
    "    print(f\"[EXPORT] Initializing merge sequence. Target: {FINAL_SAVE_DIR}...\")\n",
    "    \n",
    "    # Precision MUST be float16 for merging\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        return_dict=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        use_safetensors=True,\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n",
    "    model = model.merge_and_unload()\n",
    "    \n",
    "    model.save_pretrained(FINAL_SAVE_DIR)\n",
    "    tokenizer.save_pretrained(FINAL_SAVE_DIR)\n",
    "    print(f\"[SUCCESS] Standalone model exported to ./{FINAL_SAVE_DIR}\")\n",
    "\n",
    "export_final_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}